{"block_file": {"data_exporters/demo_exporter.py:data_exporter:python:demo exporter": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\nimport sqlite3\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#fileio\n    \"\"\"\n\n    \"\"\"\n    Initialize the SQLite connection.\n    \"\"\"\n    conn = sqlite3.connect('databases/example.db')  \n    cursor = conn.cursor()\n    tablename = \"roi_entries\"\n\n    \"\"\"\n    Create SQLite table creation command\n    \"\"\"\n    dfcols = set()\n    for field, values in df.iteritems():\n        dfcols.add(field)\n    creation_command = f\"CREATE TABLE IF NOT EXISTS {tablename} (\"\n    for field in dfcols:\n        creation_command += f\"{field} TEXT, \"\n    creation_command = creation_command[:len(creation_command) - 2] + \")\"\n\n    \"\"\"\n    Create the table if it doesn't exist\n    \"\"\"\n    print(f\"Attempting to execute: {creation_command}\")\n    cursor.execute(creation_command)\n    conn.commit()\n\n    \"\"\"\n    Create SQLite table insertion command\n    \"\"\"\n    insertion_command = f\"INSERT INTO {tablename} (\"\n    for field in dfcols:\n        insertion_command += f\"{field}, \"\n    insertion_command = insertion_command[:len(insertion_command) - 2] + \") VALUES (\"\n    for i in range(len(dfcols)):\n        insertion_command += \"?, \"\n    insertion_command = insertion_command[:len(insertion_command) - 2] + \")\"\n\n    \"\"\"\n    Write entries to the SQLite DB\n    \"\"\"\n    try:\n        count = 0\n        for index, row in df.iterrows():\n            # Retrieve entry row from DF\n            entry = []\n            for field in dfcols:\n                entry.append(row[field])\n            entry = tuple(entry)\n\n            # Insert entry row into SQLite DB\n            cursor.execute(insertion_command, entry)\n            count += 1\n            print(f'Entry inserted: {entry}')\n        conn.commit()\n        print(f'Successfully inserted {count} entries to SQLite')\n    except sqlite3.Error as e:\n        print(f\"SQLite error: {e}\")\n\n    \"\"\"\n    Close the connection to the SQLite DB\n    \"\"\"\n    conn.close()", "file_path": "data_exporters/demo_exporter.py", "language": "python", "type": "data_exporter", "uuid": "demo_exporter"}, "data_exporters/kafka_exporter.py:data_exporter:python:kafka exporter": {"content": "from team6_package.core import export_dataframe_to_kafka\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n@data_exporter\ndef export_data_to_kafka(df, *args, **kwargs):\n    #Exports data to a Kafka topic using team6_package.\n\n    #Use the package function to export data\n    export_dataframe_to_kafka(df)\n", "file_path": "data_exporters/kafka_exporter.py", "language": "python", "type": "data_exporter", "uuid": "kafka_exporter"}, "data_exporters/sqlite_exporter.py:data_exporter:python:sqlite exporter": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\nimport sqlite3\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#fileio\n    \"\"\"\n\n    \"\"\"\n    Initialize the SQLite connection.\n    \"\"\"\n    conn = sqlite3.connect('databases/example.db')  \n    cursor = conn.cursor()\n    tablename = \"investment_entries\"\n\n    \"\"\"\n    Create SQLite table creation command\n    \"\"\"\n    dfcols = set()\n    for field, values in df.iteritems():\n        dfcols.add(field)\n    creation_command = f\"CREATE TABLE IF NOT EXISTS {tablename} (\"\n    for field in dfcols:\n        creation_command += f\"{field} TEXT, \"\n    creation_command = creation_command[:len(creation_command) - 2] + \")\"\n\n    \"\"\"\n    Create the table if it doesn't exist\n    \"\"\"\n    print(f\"Attempting to execute: {creation_command}\")\n    cursor.execute(creation_command)\n    conn.commit()\n\n    \"\"\"\n    Create SQLite table insertion command\n    \"\"\"\n    insertion_command = \"INSERT INTO investment_entries (\"\n    for field in dfcols:\n        insertion_command += f\"{field}, \"\n    insertion_command = insertion_command[:len(insertion_command) - 2] + \") VALUES (\"\n    for i in range(len(dfcols)):\n        insertion_command += \"?, \"\n    insertion_command = insertion_command[:len(insertion_command) - 2] + \")\"\n\n    \"\"\"\n    Write entries to the SQLite DB\n    \"\"\"\n    try:\n        count = 0\n        for index, row in df.iterrows():\n            # Retrieve entry row from DF\n            entry = []\n            for field in dfcols:\n                entry.append(row[field])\n            entry = tuple(entry)\n\n            # Insert entry row into SQLite DB\n            cursor.execute(insertion_command, entry)\n            count += 1\n            print(f'Entry inserted: {entry}')\n        conn.commit()\n        print(f'Successfully inserted {count} entries to SQLite')\n    except sqlite3.Error as e:\n        print(f\"SQLite error: {e}\")\n\n    \"\"\"\n    Close the connection to the SQLite DB\n    \"\"\"\n    conn.close()", "file_path": "data_exporters/sqlite_exporter.py", "language": "python", "type": "data_exporter", "uuid": "sqlite_exporter"}, "data_exporters/stream_data_exporter.py:data_exporter:python:stream data exporter": {"content": "from mage_ai.streaming.sinks.base_python import BasePythonSink\nfrom typing import Dict, List\nfrom team6_package.core import create_kafka_producer\nimport logging\n\nif 'streaming_sink' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_sink\n\nlogging.basicConfig(level=logging.INFO)\n\n@streaming_sink\nclass CustomSink(BasePythonSink):\n    def init_client(self):\n        # Kafka configuration\n        self.kafka_topic = 'team6_topic'  # Update with your Kafka topic\n        self.bootstrap_servers = 'kafka:9092'  # Adjust if needed (e.g., 'localhost:29092' if running outside Docker)\n\n        # Create Kafka producer using your package function\n        self.producer = create_kafka_producer(self.bootstrap_servers)\n        logging.info(f\"Kafka producer initialized for topic '{self.kafka_topic}' at '{self.bootstrap_servers}'.\")\n\n    def batch_write(self, messages: List[Dict]):\n        for msg in messages:\n            # Handle message format\n            if isinstance(msg, dict) and 'data' in msg:\n                data = msg['data']\n            else:\n                data = msg  # Assume msg is the whole data\n\n            # Send the message to Kafka\n            try:\n                self.producer.send(self.kafka_topic, value=data)\n                logging.info(f\"Sent message to Kafka: {data}\")\n            except Exception as e:\n                logging.error(f\"Error sending message to Kafka: {e}\")\n\n        # Flush the producer to ensure all messages are sent\n        self.producer.flush()\n        logging.info(\"Kafka producer flushed.\")\n\n    def __del__(self):\n        \"\"\"\n        Clean up resources when the sink is destroyed.\n        \"\"\"\n        if hasattr(self, 'producer'):\n            self.producer.close()\n            logging.info(\"Kafka producer closed.\")", "file_path": "data_exporters/stream_data_exporter.py", "language": "python", "type": "data_exporter", "uuid": "stream_data_exporter"}, "data_exporters/team6_data_to_kafka.yaml:data_exporter:yaml:team6 data to kafka": {"content": "connector_type: kafka\nbootstrap_server: \"kafka:9092\"\ntopic: team6_topic\napi_version: 3.8.0\n\n# Uncomment the config below to use SSL config\n# security_protocol: \"SSL\"\n# ssl_config:\n#   cafile: \"CARoot.pem\"\n#   certfile: \"certificate.pem\"\n#   keyfile: \"key.pem\"\n#   password: password\n#   check_hostname: true\n\n# Uncomment the config below to use SASL_SSL config\n# security_protocol: \"SASL_SSL\"\n# sasl_config:\n#   mechanism: \"PLAIN\"\n#   username: username\n#   password: password\n", "file_path": "data_exporters/team6_data_to_kafka.yaml", "language": "yaml", "type": "data_exporter", "uuid": "team6_data_to_kafka"}, "data_loaders/data_generator.py:data_loader:python:data generator": {"content": "import io\nimport pandas as pd\nimport requests\nfrom team6_package import generate_data, save_to_csv, load_schema\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    # Load schema from schema templates folder\n    schema = load_schema('/home/src/schemas/schema.json')\n\n    # Generate batch of data according to schema\n    data = generate_data(schema, num_records=5) #num_records is how many records will be generated.\n\n    # Save data as StringIO object\n    csv = save_to_csv(data)\n\n    return pd.read_csv(csv, sep=',')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/data_generator.py", "language": "python", "type": "data_loader", "uuid": "data_generator"}, "data_loaders/demo_loader.py:data_loader:python:demo loader": {"content": "import sqlalchemy\nimport pandas as pd\nfrom data_introspection import create_introspector\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Template code for loading data from any source.\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n\n    # View the existing tables in the sqlite database\n    introspector = create_introspector('sqlite', db_path=\"/home/src/databases/example.db\")\n    tables = introspector.get_table_names()\n    print(f\"Tables found in source database: {tables}\")\n\n    # Load a table from the database for transformation\n    return pd.read_sql('investment_entries', 'sqlite:////home/src/databases/example.db')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/demo_loader.py", "language": "python", "type": "data_loader", "uuid": "demo_loader"}, "data_loaders/generator.py:data_loader:python:generator": {"content": "from mage_ai.streaming.sources.base_python import BasePythonSource\nfrom typing import Callable\nimport random\nimport time\n\nif 'streaming_source' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_source\n\npossible_names = ['Scarlett', 'Melody', 'Bennett', 'Drew', 'Olivia', 'Mariah', 'Jose', 'Levi', 'Nikhil', 'Quinn']\npossible_grades = ['A','B','C','D','E','F']\n\n# NOTE: edit interval_seconds to change how often (in seconds) records are generated\ninterval_seconds = 30\n\n# NOTE: edit batch_size to change how many records are generated per time interval\nbatch_size = 3\n\n@streaming_source\nclass CustomSource(BasePythonSource):\n    def init_client(self):\n        \"\"\"\n        Implement the logic of initializing the client.\n        \"\"\"\n\n    def batch_read(self, handler: Callable):\n        \"\"\"\n        Batch read the messages from the source and use handler to process the messages.\n        \"\"\"\n        num_names = len(possible_names)\n        num_grades = len(possible_grades)\n        while True:\n            records = []\n            # Implement the logic of fetching the records\n\n            # Generate random name/grade records\n            for i in range(batch_size):\n                name = possible_names[random.randrange(0,num_names)]\n                grade = possible_grades[random.randrange(0,num_grades)]\n                new_record = {\"data\": {\"name\": name, \"grade\": grade}}\n                records.append(new_record)\n\n            if len(records) > 0:\n                handler(records)\n            \n            # Sleep in between every set of records generated and sent\n            time.sleep(interval_seconds)\n", "file_path": "data_loaders/generator.py", "language": "python", "type": "data_loader", "uuid": "generator"}, "data_loaders/investment_generator.py:data_loader:python:investment generator": {"content": "import io\nimport pandas as pd\nimport requests\nfrom team6_package import generate_data, save_to_csv, load_schema\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    # Load schema from schema templates folder\n    schema = load_schema('/home/src/schemas/investment_schema.json')\n\n    # Generate batch of data according to schema\n    data = generate_data(schema, num_records=5)\n\n    # Save data as StringIO object\n    csv = save_to_csv(data)\n\n    return pd.read_csv(csv, sep=',')\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "data_loaders/investment_generator.py", "language": "python", "type": "data_loader", "uuid": "investment_generator"}, "data_loaders/stream_batch_data_generator.py:data_loader:python:stream batch data generator": {"content": "from mage_ai.streaming.sources.base_python import BasePythonSource\nfrom typing import Callable\nfrom team6_package.core import generate_batch_with_time_intervals, load_schema\nimport logging\nfrom datetime import datetime, timedelta\nimport time\n\nif 'streaming_source' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_source\n\nlogging.basicConfig(level=logging.INFO)\n\n@streaming_source\nclass CustomSource(BasePythonSource):\n    def init_client(self):\n        # Load the schema\n        self.schema = load_schema('/home/src/schemas/schema.json')\n        \n        # Set parameters for data generation\n        self.interval_between_batches = 10  # Time between batches in seconds\n        self.records_per_batch = 5  # Number of records per batch\n        self.record_interval_seconds = 5  # Interval between records in a batch in seconds\n        self.total_batches = None  # Set to None for indefinite streaming\n        self.batch_count = 0\n        self.start_time = None  # Set to specific datetime if needed. If None, uses current time in UTC.\n        self.current_time = self.start_time  # Initialize current_time here\n\n    def batch_read(self, handler: Callable):\n        \"\"\"\n        Batch read the messages from the source and use handler to process the messages.\n        \"\"\"\n        logging.info(\"Starting batch_read in CustomSource.\")\n        try:\n            while True:\n                # Generate the batch with time intervals\n                records = generate_batch_with_time_intervals(\n                    self.schema,\n                    self.records_per_batch,\n                    start_time=self.current_time,\n                    interval_seconds=self.record_interval_seconds\n                )\n\n                if records:\n                    handler(records)\n                    self.batch_count += 1\n                    logging.info(f\"Generated and processed batch {self.batch_count} with {len(records)} records.\")\n\n                    # Update self.current_time for the next batch\n                    datetime_field = next(\n                        (key for key in records[-1] if self.schema[key].lower() == 'datetime'), None\n                    )\n                    if datetime_field:\n                        last_record_time_str = records[-1][datetime_field]\n                        self.current_time = datetime.strptime(\n                            last_record_time_str, '%Y-%m-%d %H:%M:%S'\n                        ) + timedelta(seconds=self.record_interval_seconds)\n                    else:\n                        self.current_time = datetime.now()\n                else:\n                    logging.warning(\"No records generated in this batch.\")\n                    self.current_time = datetime.now()\n\n                if self.total_batches is not None and self.batch_count >= self.total_batches:\n                    logging.info(\"Reached total number of batches to send.\")\n                    break\n\n                time.sleep(self.interval_between_batches)\n\n        except Exception as e:\n            logging.error(f\"Error during data streaming: {e}\", exc_info=True)\n        finally:\n            logging.info(\"Data streaming completed.\")", "file_path": "data_loaders/stream_batch_data_generator.py", "language": "python", "type": "data_loader", "uuid": "stream_batch_data_generator"}, "data_loaders/stream_data_generator.py:data_loader:python:stream data generator": {"content": "from mage_ai.streaming.sources.base_python import BasePythonSource\nfrom typing import Callable\nfrom team6_package.core import generate_data, load_schema\nimport time\nimport logging\n\nif 'streaming_source' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_source\n\nlogging.basicConfig(level=logging.INFO)\n\n@streaming_source\nclass CustomSource(BasePythonSource):\n    def init_client(self):\n        # Load the schema\n        self.schema = load_schema('/home/src/schemas/schema.json')\n        \n        # Set parameters for data generation\n        self.interval = 5.0  # Interval between batches in seconds\n        self.records_per_batch = 1  # 1 IF YOU WANT A CONSTANT STREAM OF SINGLE RECORDS. FOR MORE THAN 1 USE THE STREAM_BATCH_DATA_GENERATOR INSTEAD\n        self.total_batches = None  # Set to None for indefinite streaming\n        self.batch_count = 0\n\n    def batch_read(self, handler: Callable):\n        \"\"\"\n        Batch read the messages from the source and use handler to process the messages.\n        \"\"\"\n        try:\n            while True:\n                records = generate_data(self.schema, self.records_per_batch)\n                if len(records) > 0:\n                    handler(records)\n                    logging.info(f\"Generated and processed batch {self.batch_count + 1} with {len(records)} records.\")\n                self.batch_count += 1\n                if self.total_batches is not None and self.batch_count >= self.total_batches:\n                    logging.info(\"Reached total number of batches to send.\")\n                    break\n                time.sleep(self.interval)\n        except Exception as e:\n            logging.error(f\"Error during data streaming: {e}\")\n        finally:\n            logging.info(\"Data streaming completed.\")", "file_path": "data_loaders/stream_data_generator.py", "language": "python", "type": "data_loader", "uuid": "stream_data_generator"}, "data_loaders/stream_loader_team6.py:data_loader:python:stream loader team6": {"content": "from mage_ai.streaming.sources.base_python import BasePythonSource\nfrom typing import Callable\n\nif 'streaming_source' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_source\n\n\n@streaming_source\nclass CustomSource(BasePythonSource):\n    def init_client(self):\n        \"\"\"\n        Implement the logic of initializing the client.\n        \"\"\"\n\n    def batch_read(self, handler: Callable):\n        \"\"\"\n        Batch read the messages from the source and use handler to process the messages.\n        \"\"\"\n        while True:\n            records = []\n            # Implement the logic of fetching the records\n            if len(records) > 0:\n                handler(records)\n", "file_path": "data_loaders/stream_loader_team6.py", "language": "python", "type": "data_loader", "uuid": "stream_loader_team6"}, "transformers/demo_transformer.py:transformer:python:demo transformer": {"content": "from kpi_formula.advanced.kpi_calculator import KPICalculator\nfrom pandas import DataFrame\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data: DataFrame, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n\n    ROIs = []\n    for index, row in data.iterrows():\n        # Calculate return on investment (ROI) for each entry\n        ROI = KPICalculator.roi(revenue=int(row['revenue']), investment=int(row['initial_investment']))\n        ROIs.append(ROI)\n\n    # Add ROI as column in DataFrame\n    data['roi'] = ROIs\n\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/demo_transformer.py", "language": "python", "type": "transformer", "uuid": "demo_transformer"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "pipelines/batch_data_generation/metadata.yaml:pipeline:yaml:batch data generation/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/data_generator.py\n  downstream_blocks:\n  - kafka_exporter\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: data_generator\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: data_generator\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/kafka_exporter.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: kafka_exporter\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - data_generator\n  uuid: kafka_exporter\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-11-14 04:10:05.012046+00:00'\ndata_integration: null\ndescription: Generate a batch of data and send it to a kafka topic using team 6's\n  package\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: Batch_Data_Generation\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: batch_data_generation\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/batch_data_generation/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "batch_data_generation/metadata"}, "pipelines/batch_data_generation/__init__.py:pipeline:python:batch data generation/  init  ": {"content": "", "file_path": "pipelines/batch_data_generation/__init__.py", "language": "python", "type": "pipeline", "uuid": "batch_data_generation/__init__"}, "pipelines/batch_proof_of_concept/metadata.yaml:pipeline:yaml:batch proof of concept/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - kafka_exporter\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: data_generator\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: data_generator\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_path: data_exporters/kafka_exporter.py\n    file_source:\n      path: data_exporters/kafka_exporter.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: kafka_exporter\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - data_generator\n<<<<<<<< HEAD:mage/default_repo/pipelines/team6_integration_demo/metadata.yaml\n  uuid: sqlite_exporter\n========\n  uuid: kafka_exporter\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: kpi_testing\n  retry_config: null\n  status: updated\n  timeout: null\n  type: scratchpad\n  upstream_blocks: []\n  uuid: kpi_testing\n>>>>>>>> 0a07ab4a6c660b33b89455f32c2c45359a088ff1:mage/default_repo/pipelines/batch_proof_of_concept/metadata.yaml\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-11-04 15:30:40.241176+00:00'\ndata_integration: null\ndescription: 'proof of concept: generate data with Team 6''s Python module and store\n  in a SQLite DB'\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\n<<<<<<<< HEAD:mage/default_repo/pipelines/team6_integration_demo/metadata.yaml\nname: team6_integration_demo\n========\nname: batch_proof_of_concept\n>>>>>>>> 0a07ab4a6c660b33b89455f32c2c45359a088ff1:mage/default_repo/pipelines/batch_proof_of_concept/metadata.yaml\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\n<<<<<<<< HEAD:mage/default_repo/pipelines/team6_integration_demo/metadata.yaml\nuuid: team6_integration_demo\n========\nuuid: batch_proof_of_concept\n>>>>>>>> 0a07ab4a6c660b33b89455f32c2c45359a088ff1:mage/default_repo/pipelines/batch_proof_of_concept/metadata.yaml\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/batch_proof_of_concept/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "batch_proof_of_concept/metadata"}, "pipelines/batch_proof_of_concept/__init__.py:pipeline:python:batch proof of concept/  init  ": {"content": "", "file_path": "pipelines/batch_proof_of_concept/__init__.py", "language": "python", "type": "pipeline", "uuid": "batch_proof_of_concept/__init__"}, "pipelines/data_generation_demo/metadata.yaml:pipeline:yaml:data generation demo/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/investment_generator.py\n    file_source:\n      path: data_loaders/investment_generator.py\n  downstream_blocks:\n  - sqlite_exporter\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: investment_generator\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: investment_generator\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/sqlite_exporter.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: sqlite_exporter\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - investment_generator\n  uuid: sqlite_exporter\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/demo_exporter.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: demo_exporter\n  retry_config: null\n  status: not_executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks: []\n  uuid: demo_exporter\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-11-15 02:39:22.977506+00:00'\ndata_integration: null\ndescription: store example batch data generated by team 6 module in database/example.db\n  (used for integration_demo)\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: data_generation_demo\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: data_generation_demo\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/data_generation_demo/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "data_generation_demo/metadata"}, "pipelines/data_generation_demo/__init__.py:pipeline:python:data generation demo/  init  ": {"content": "", "file_path": "pipelines/data_generation_demo/__init__.py", "language": "python", "type": "pipeline", "uuid": "data_generation_demo/__init__"}, "pipelines/integration_demo/metadata.yaml:pipeline:yaml:integration demo/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - demo_transformer\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: demo_loader\n  retry_config: null\n  status: failed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: demo_loader\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - demo_exporter\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: demo_transformer\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - demo_loader\n  uuid: demo_transformer\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: demo_exporter\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - demo_transformer\n  uuid: demo_exporter\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-11-22 13:25:31.311997+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: integration_demo\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: integration_demo\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/integration_demo/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "integration_demo/metadata"}, "pipelines/integration_demo/__init__.py:pipeline:python:integration demo/  init  ": {"content": "", "file_path": "pipelines/integration_demo/__init__.py", "language": "python", "type": "pipeline", "uuid": "integration_demo/__init__"}, "pipelines/stream_data_generator/metadata.yaml:pipeline:yaml:stream data generator/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/stream_data_generator.py\n  downstream_blocks:\n  - team6_data_to_kafka\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: stream_data_generator\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: stream_data_generator\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: team6_data_to_kafka\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - stream_data_generator\n  uuid: team6_data_to_kafka\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-11-14 04:48:48.602145+00:00'\ndata_integration: null\ndescription: generate a stream (single or batch) of data and export it to a kafka\n  topic.\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: stream_data_generator\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: streaming\nuuid: stream_data_generator\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/stream_data_generator/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "stream_data_generator/metadata"}, "pipelines/stream_data_generator/__init__.py:pipeline:python:stream data generator/  init  ": {"content": "", "file_path": "pipelines/stream_data_generator/__init__.py", "language": "python", "type": "pipeline", "uuid": "stream_data_generator/__init__"}, "pipelines/stream_of_batches_data_generator/metadata.yaml:pipeline:yaml:stream of batches data generator/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - team6_data_to_kafka\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: stream_batch_data_generator\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: stream_batch_data_generator\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/team6_data_to_kafka.yaml\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: team6_data_to_kafka\n  retry_config: null\n  status: not_executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - stream_batch_data_generator\n  uuid: team6_data_to_kafka\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-11-14 06:00:13.109679+00:00'\ndata_integration: null\ndescription: generate a stream of batches and send them to kafka topic using team\n  6's package\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: stream_of_batches_data_generator\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: streaming\nuuid: stream_of_batches_data_generator\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/stream_of_batches_data_generator/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "stream_of_batches_data_generator/metadata"}, "pipelines/stream_of_batches_data_generator/__init__.py:pipeline:python:stream of batches data generator/  init  ": {"content": "", "file_path": "pipelines/stream_of_batches_data_generator/__init__.py", "language": "python", "type": "pipeline", "uuid": "stream_of_batches_data_generator/__init__"}, "pipelines/team6_integration_demo/metadata.yaml:pipeline:yaml:team6 integration demo/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - kafka_exporter\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: data_generator\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: data_generator\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_path: data_exporters/kafka_exporter.py\n    file_source:\n      path: data_exporters/kafka_exporter.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: kafka_exporter\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - data_generator\n<<<<<<<< HEAD:mage/default_repo/pipelines/team6_integration_demo/metadata.yaml\n  uuid: sqlite_exporter\n========\n  uuid: kafka_exporter\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: kpi_testing\n  retry_config: null\n  status: updated\n  timeout: null\n  type: scratchpad\n  upstream_blocks: []\n  uuid: kpi_testing\n>>>>>>>> 0a07ab4a6c660b33b89455f32c2c45359a088ff1:mage/default_repo/pipelines/batch_proof_of_concept/metadata.yaml\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-11-04 15:30:40.241176+00:00'\ndata_integration: null\ndescription: 'proof of concept: generate data with Team 6''s Python module and store\n  in a SQLite DB'\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\n<<<<<<<< HEAD:mage/default_repo/pipelines/team6_integration_demo/metadata.yaml\nname: team6_integration_demo\n========\nname: batch_proof_of_concept\n>>>>>>>> 0a07ab4a6c660b33b89455f32c2c45359a088ff1:mage/default_repo/pipelines/batch_proof_of_concept/metadata.yaml\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\n<<<<<<<< HEAD:mage/default_repo/pipelines/team6_integration_demo/metadata.yaml\nuuid: team6_integration_demo\n========\nuuid: batch_proof_of_concept\n>>>>>>>> 0a07ab4a6c660b33b89455f32c2c45359a088ff1:mage/default_repo/pipelines/batch_proof_of_concept/metadata.yaml\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/team6_integration_demo/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "team6_integration_demo/metadata"}, "pipelines/team6_integration_demo/__init__.py:pipeline:python:team6 integration demo/  init  ": {"content": "", "file_path": "pipelines/team6_integration_demo/__init__.py", "language": "python", "type": "pipeline", "uuid": "team6_integration_demo/__init__"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}